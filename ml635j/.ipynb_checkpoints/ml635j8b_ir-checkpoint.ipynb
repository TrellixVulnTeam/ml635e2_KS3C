{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FINE TUNING FASTER RCNN USING PYTORCH**\n",
    "\n",
    "6 os \n",
    "\n",
    "In this Notebook we can fine tune a Faster RCNN on the images dataset. If you want to brush up about what is Faster RCNN, [here's](https://medium.com/@whatdhack/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd) an awesome medium article on the same.\n",
    "\n",
    "Ref: inspired by the Pytorch docs tutorial [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a lot of code for object detection is same and has to be rewritten by everyone, torchvision contributers have provided us with helper codes for training, evaluation and transformations.\n",
    "\n",
    "Let's clone the repo and copy the libraries into working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TorchVision repo to use some files from references/detection\n",
    "# !git clone https://github.com/pytorch/vision.git\n",
    "# !git checkout v0.3.0\n",
    "# !cp vision/references/detection/utils.py ./\n",
    "# !cp vision/references/detection/transforms.py ./\n",
    "# !cp vision/references/detection/coco_eval.py ./\n",
    "# !cp vision/references/detection/engine.py ./\n",
    "# !cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# apt update\n",
    "# DEBIAN_FRONTEND=noninteractive apt-get install -y python3-opencv\n",
    "# #tz prompt. dpkg fix...\n",
    "# pip install opencv-python\n",
    "# #pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- ---------\n",
      "absl-py                 1.0.0\n",
      "albumentations          1.1.0\n",
      "argcomplete             1.12.3\n",
      "argon2-cffi             21.1.0\n",
      "attrs                   21.2.0\n",
      "backcall                0.2.0\n",
      "beautifulsoup4          4.10.0\n",
      "bleach                  4.1.0\n",
      "blis                    0.7.5\n",
      "boto3                   1.20.4\n",
      "botocore                1.23.4\n",
      "bravado                 11.0.3\n",
      "bravado-core            5.17.0\n",
      "brotlipy                0.7.0\n",
      "cachetools              4.2.4\n",
      "captum                  0.4.1\n",
      "catalogue               2.0.6\n",
      "catalyst                21.10\n",
      "certifi                 2021.10.8\n",
      "cffi                    1.14.6\n",
      "chardet                 4.0.0\n",
      "click                   8.0.3\n",
      "conda                   4.10.3\n",
      "conda-build             3.21.5\n",
      "conda-package-handling  1.7.3\n",
      "configparser            5.1.0\n",
      "cryptography            3.4.8\n",
      "cycler                  0.11.0\n",
      "cymem                   2.0.6\n",
      "debugpy                 1.5.1\n",
      "decorator               5.1.0\n",
      "defusedxml              0.7.1\n",
      "dnspython               2.1.0\n",
      "docker-pycreds          0.4.0\n",
      "entrypoints             0.3\n",
      "fastai                  2.5.4\n",
      "fastcore                1.3.28\n",
      "fastdownload            0.0.5\n",
      "fastprogress            1.0.0\n",
      "fastrelease             0.1.12\n",
      "filelock                3.0.12\n",
      "future                  0.18.2\n",
      "ghapi                   0.1.19\n",
      "gitdb                   4.0.9\n",
      "GitPython               3.1.24\n",
      "glob2                   0.7\n",
      "google-auth             2.3.3\n",
      "google-auth-oauthlib    0.4.6\n",
      "graphviz                0.18\n",
      "grpcio                  1.41.1\n",
      "hydra-slayer            0.3.0\n",
      "idna                    2.10\n",
      "imageio                 2.10.3\n",
      "importlib-metadata      4.8.2\n",
      "ipykernel               6.5.0\n",
      "ipython                 7.27.0\n",
      "ipython-genutils        0.2.0\n",
      "ipywidgets              7.6.5\n",
      "jedi                    0.18.0\n",
      "Jinja2                  2.11.3\n",
      "jmespath                0.10.0\n",
      "joblib                  1.1.0\n",
      "jsonpointer             2.2\n",
      "jsonref                 0.2\n",
      "jsonschema              3.2.0\n",
      "jupyter                 1.0.0\n",
      "jupyter-client          6.1.12\n",
      "jupyter-console         6.4.0\n",
      "jupyter-core            4.9.1\n",
      "jupyterlab-widgets      1.0.2\n",
      "kiwisolver              1.3.2\n",
      "kornia                  0.6.1\n",
      "langcodes               3.3.0\n",
      "libarchive-c            2.9\n",
      "Markdown                3.3.4\n",
      "MarkupSafe              2.0.1\n",
      "matplotlib              3.4.3\n",
      "matplotlib-inline       0.1.2\n",
      "mistune                 0.8.4\n",
      "mkl-fft                 1.3.1\n",
      "mkl-random              1.2.2\n",
      "mkl-service             2.4.0\n",
      "monotonic               1.6\n",
      "msgpack                 1.0.2\n",
      "murmurhash              1.0.6\n",
      "nbconvert               5.6.1\n",
      "nbdev                   1.1.23\n",
      "nbformat                5.1.3\n",
      "neptune-client          0.13.1\n",
      "networkx                2.6.3\n",
      "notebook                6.4.5\n",
      "numpy                   1.21.2\n",
      "oauthlib                3.1.1\n",
      "olefile                 0.46\n",
      "opencv-python           4.5.4.58\n",
      "opencv-python-headless  4.5.4.58\n",
      "packaging               21.2\n",
      "pandas                  1.3.4\n",
      "pandocfilters           1.5.0\n",
      "parso                   0.8.2\n",
      "pathtools               0.1.2\n",
      "pathy                   0.6.1\n",
      "pexpect                 4.8.0\n",
      "pickleshare             0.7.5\n",
      "Pillow                  8.4.0\n",
      "pip                     21.0.1\n",
      "pkginfo                 1.7.1\n",
      "preshed                 3.0.6\n",
      "prometheus-client       0.12.0\n",
      "promise                 2.3\n",
      "prompt-toolkit          3.0.20\n",
      "protobuf                3.19.1\n",
      "psutil                  5.8.0\n",
      "ptyprocess              0.7.0\n",
      "pyarrow                 6.0.0\n",
      "pyasn1                  0.4.8\n",
      "pyasn1-modules          0.2.8\n",
      "pycocotools             2.0.4\n",
      "pycosat                 0.6.3\n",
      "pycparser               2.20\n",
      "pydantic                1.8.2\n",
      "pydicom                 2.2.2\n",
      "Pygments                2.10.0\n",
      "PyJWT                   2.3.0\n",
      "pyOpenSSL               20.0.1\n",
      "pyparsing               2.4.7\n",
      "pyrsistent              0.18.0\n",
      "PySocks                 1.7.1\n",
      "python-dateutil         2.8.2\n",
      "python-etcd             0.4.5\n",
      "pytz                    2021.3\n",
      "PyWavelets              1.2.0\n",
      "PyYAML                  5.4.1\n",
      "pyzmq                   22.3.0\n",
      "qtconsole               5.2.0\n",
      "QtPy                    1.11.2\n",
      "qudida                  0.0.4\n",
      "requests                2.25.1\n",
      "requests-oauthlib       1.3.0\n",
      "rfc3987                 1.3.8\n",
      "rsa                     4.7.2\n",
      "ruamel-yaml-conda       0.15.100\n",
      "s3transfer              0.5.0\n",
      "scikit-image            0.18.3\n",
      "scikit-learn            1.0.1\n",
      "scipy                   1.7.2\n",
      "Send2Trash              1.8.0\n",
      "sentencepiece           0.1.86\n",
      "sentry-sdk              1.4.3\n",
      "setuptools              58.0.4\n",
      "shortuuid               1.0.8\n",
      "simplejson              3.17.5\n",
      "six                     1.16.0\n",
      "smart-open              5.2.1\n",
      "smmap                   5.0.0\n",
      "soupsieve               2.2.1\n",
      "spacy                   3.2.0\n",
      "spacy-legacy            3.0.8\n",
      "spacy-loggers           1.0.1\n",
      "srsly                   2.4.2\n",
      "strict-rfc3339          0.7\n",
      "subprocess32            3.5.4\n",
      "swagger-spec-validator  2.7.4\n",
      "tensorboard             2.7.0\n",
      "tensorboard-data-server 0.6.1\n",
      "tensorboard-plugin-wit  1.8.0\n",
      "tensorboardX            2.2\n",
      "termcolor               1.1.0\n",
      "terminado               0.12.1\n",
      "testpath                0.5.0\n",
      "thinc                   8.0.13\n",
      "threadpoolctl           3.0.0\n",
      "tifffile                2021.11.2\n",
      "torch                   1.10.0\n",
      "torchelastic            0.2.0\n",
      "torchtext               0.11.0\n",
      "torchvision             0.11.0\n",
      "tornado                 6.1\n",
      "tqdm                    4.61.2\n",
      "traitlets               5.1.0\n",
      "typer                   0.4.0\n",
      "typing-extensions       3.10.0.2\n",
      "urllib3                 1.26.6\n",
      "wandb                   0.12.6\n",
      "wasabi                  0.8.2\n",
      "wcwidth                 0.2.5\n",
      "webcolors               1.11.1\n",
      "webencodings            0.5.1\n",
      "websocket-client        1.2.1\n",
      "Werkzeug                2.0.2\n",
      "wheel                   0.36.2\n",
      "widgetsnbextension      3.5.2\n",
      "yaspin                  2.1.0\n",
      "zipp                    3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /opt/conda:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "_libgcc_mutex             0.1                        main  \r\n",
      "_openmp_mutex             4.5                       1_gnu  \r\n",
      "absl-py                   1.0.0                    pypi_0    pypi\r\n",
      "albumentations            1.1.0                    pypi_0    pypi\r\n",
      "argcomplete               1.12.3                   pypi_0    pypi\r\n",
      "argon2-cffi               21.1.0                   pypi_0    pypi\r\n",
      "attrs                     21.2.0                   pypi_0    pypi\r\n",
      "backcall                  0.2.0              pyhd3eb1b0_0  \r\n",
      "beautifulsoup4            4.10.0             pyh06a4308_0  \r\n",
      "blas                      1.0                         mkl  \r\n",
      "bleach                    4.1.0                    pypi_0    pypi\r\n",
      "blis                      0.7.5                    pypi_0    pypi\r\n",
      "boto3                     1.20.4                   pypi_0    pypi\r\n",
      "botocore                  1.23.4                   pypi_0    pypi\r\n",
      "bravado                   11.0.3                   pypi_0    pypi\r\n",
      "bravado-core              5.17.0                   pypi_0    pypi\r\n",
      "brotlipy                  0.7.0           py37h27cfd23_1003  \r\n",
      "bzip2                     1.0.8                h7b6447c_0  \r\n",
      "ca-certificates           2021.9.30            h06a4308_1  \r\n",
      "cachetools                4.2.4                    pypi_0    pypi\r\n",
      "captum                    0.4.1                    pypi_0    pypi\r\n",
      "catalogue                 2.0.6                    pypi_0    pypi\r\n",
      "catalyst                  21.10                    pypi_0    pypi\r\n",
      "certifi                   2021.10.8        py37h06a4308_0  \r\n",
      "cffi                      1.14.6           py37h400218f_0  \r\n",
      "chardet                   4.0.0           py37h06a4308_1003  \r\n",
      "click                     8.0.3                    pypi_0    pypi\r\n",
      "conda                     4.10.3           py37h06a4308_0  \r\n",
      "conda-build               3.21.5           py37h06a4308_0  \r\n",
      "conda-package-handling    1.7.3            py37h27cfd23_1  \r\n",
      "configparser              5.1.0                    pypi_0    pypi\r\n",
      "cryptography              3.4.8            py37hd23ed53_0  \r\n",
      "cudatoolkit               11.3.1               ha36c431_9    nvidia\r\n",
      "cycler                    0.11.0                   pypi_0    pypi\r\n",
      "cymem                     2.0.6                    pypi_0    pypi\r\n",
      "debugpy                   1.5.1                    pypi_0    pypi\r\n",
      "decorator                 5.1.0              pyhd3eb1b0_0  \r\n",
      "defusedxml                0.7.1                    pypi_0    pypi\r\n",
      "dnspython                 2.1.0                    pypi_0    pypi\r\n",
      "docker-pycreds            0.4.0                    pypi_0    pypi\r\n",
      "entrypoints               0.3                      pypi_0    pypi\r\n",
      "fastai                    2.5.4                    pypi_0    pypi\r\n",
      "fastcore                  1.3.28                   pypi_0    pypi\r\n",
      "fastdownload              0.0.5                    pypi_0    pypi\r\n",
      "fastprogress              1.0.0                    pypi_0    pypi\r\n",
      "fastrelease               0.1.12                   pypi_0    pypi\r\n",
      "ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n",
      "filelock                  3.0.12             pyhd3eb1b0_1  \r\n",
      "freetype                  2.10.4               h5ab3b9f_0  \r\n",
      "future                    0.18.2                   pypi_0    pypi\r\n",
      "ghapi                     0.1.19                   pypi_0    pypi\r\n",
      "giflib                    5.2.1                h7b6447c_0  \r\n",
      "gitdb                     4.0.9                    pypi_0    pypi\r\n",
      "gitpython                 3.1.24                   pypi_0    pypi\r\n",
      "glob2                     0.7                pyhd3eb1b0_0  \r\n",
      "gmp                       6.2.1                h2531618_2  \r\n",
      "gnutls                    3.6.15               he1e5248_0  \r\n",
      "google-auth               2.3.3                    pypi_0    pypi\r\n",
      "google-auth-oauthlib      0.4.6                    pypi_0    pypi\r\n",
      "grpcio                    1.41.1                   pypi_0    pypi\r\n",
      "hydra-slayer              0.3.0                    pypi_0    pypi\r\n",
      "icu                       58.2                 he6710b0_3  \r\n",
      "idna                      2.10               pyhd3eb1b0_0  \r\n",
      "imageio                   2.10.3                   pypi_0    pypi\r\n",
      "importlib-metadata        4.8.2                    pypi_0    pypi\r\n",
      "intel-openmp              2021.3.0          h06a4308_3350  \r\n",
      "ipykernel                 6.5.0                    pypi_0    pypi\r\n",
      "ipython                   7.27.0           py37hb070fc8_0  \r\n",
      "ipython-genutils          0.2.0                    pypi_0    pypi\r\n",
      "ipywidgets                7.6.5                    pypi_0    pypi\r\n",
      "jedi                      0.18.0           py37h06a4308_1  \r\n",
      "jinja2                    2.11.3             pyhd3eb1b0_0  \r\n",
      "jmespath                  0.10.0                   pypi_0    pypi\r\n",
      "joblib                    1.1.0                    pypi_0    pypi\r\n",
      "jpeg                      9d                   h7f8727e_0  \r\n",
      "jsonpointer               2.2                      pypi_0    pypi\r\n",
      "jsonref                   0.2                      pypi_0    pypi\r\n",
      "jsonschema                3.2.0                    pypi_0    pypi\r\n",
      "jupyter                   1.0.0                    pypi_0    pypi\r\n",
      "jupyter-client            6.1.12                   pypi_0    pypi\r\n",
      "jupyter-console           6.4.0                    pypi_0    pypi\r\n",
      "jupyter-core              4.9.1                    pypi_0    pypi\r\n",
      "jupyterlab-widgets        1.0.2                    pypi_0    pypi\r\n",
      "kiwisolver                1.3.2                    pypi_0    pypi\r\n",
      "kornia                    0.6.1                    pypi_0    pypi\r\n",
      "lame                      3.100                h7b6447c_0  \r\n",
      "langcodes                 3.3.0                    pypi_0    pypi\r\n",
      "lcms2                     2.12                 h3be6417_0  \r\n",
      "ld_impl_linux-64          2.35.1               h7274673_9  \r\n",
      "libarchive                3.4.2                h62408e4_0  \r\n",
      "libffi                    3.3                  he6710b0_2  \r\n",
      "libgcc-ng                 9.3.0               h5101ec6_17  \r\n",
      "libgomp                   9.3.0               h5101ec6_17  \r\n",
      "libiconv                  1.15                 h63c8f33_5  \r\n",
      "libidn2                   2.3.2                h7f8727e_0  \r\n",
      "liblief                   0.10.1               he6710b0_0  \r\n",
      "libpng                    1.6.37               hbc83047_0  \r\n",
      "libstdcxx-ng              9.3.0               hd4cf53a_17  \r\n",
      "libtasn1                  4.16.0               h27cfd23_0  \r\n",
      "libtiff                   4.2.0                h85742a9_0  \r\n",
      "libunistring              0.9.10               h27cfd23_0  \r\n",
      "libuv                     1.40.0               h7b6447c_0  \r\n",
      "libwebp                   1.2.0                h89dd481_0  \r\n",
      "libwebp-base              1.2.0                h27cfd23_0  \r\n",
      "libxml2                   2.9.12               h03d6c58_0  \r\n",
      "lz4-c                     1.9.3                h295c915_1  \r\n",
      "markdown                  3.3.4                    pypi_0    pypi\r\n",
      "markupsafe                2.0.1            py37h27cfd23_0  \r\n",
      "matplotlib                3.4.3                    pypi_0    pypi\r\n",
      "matplotlib-inline         0.1.2              pyhd3eb1b0_2  \r\n",
      "mistune                   0.8.4                    pypi_0    pypi\r\n",
      "mkl                       2021.3.0           h06a4308_520  \r\n",
      "mkl-service               2.4.0            py37h7f8727e_0  \r\n",
      "mkl_fft                   1.3.1            py37hd3c417c_0  \r\n",
      "mkl_random                1.2.2            py37h51133e4_0  \r\n",
      "monotonic                 1.6                      pypi_0    pypi\r\n",
      "msgpack                   1.0.2                    pypi_0    pypi\r\n",
      "murmurhash                1.0.6                    pypi_0    pypi\r\n",
      "nbconvert                 5.6.1                    pypi_0    pypi\r\n",
      "nbdev                     1.1.23                   pypi_0    pypi\r\n",
      "nbformat                  5.1.3                    pypi_0    pypi\r\n",
      "ncurses                   6.2                  he6710b0_1  \r\n",
      "neptune-client            0.13.1                   pypi_0    pypi\r\n",
      "nettle                    3.7.3                hbbd107a_1  \r\n",
      "networkx                  2.6.3                    pypi_0    pypi\r\n",
      "notebook                  6.4.5                    pypi_0    pypi\r\n",
      "numpy                     1.21.2           py37h20f2e39_0  \r\n",
      "numpy-base                1.21.2           py37h79a1101_0  \r\n",
      "oauthlib                  3.1.1                    pypi_0    pypi\r\n",
      "olefile                   0.46                     py37_0  \r\n",
      "opencv-python             4.5.4.58                 pypi_0    pypi\r\n",
      "opencv-python-headless    4.5.4.58                 pypi_0    pypi\r\n",
      "openh264                  2.1.0                hd408876_0  \r\n",
      "openssl                   1.1.1l               h7f8727e_0  \r\n",
      "packaging                 21.2                     pypi_0    pypi\r\n",
      "pandas                    1.3.4                    pypi_0    pypi\r\n",
      "pandocfilters             1.5.0                    pypi_0    pypi\r\n",
      "parso                     0.8.2              pyhd3eb1b0_0  \r\n",
      "patchelf                  0.13                 h295c915_0  \r\n",
      "pathtools                 0.1.2                    pypi_0    pypi\r\n",
      "pathy                     0.6.1                    pypi_0    pypi\r\n",
      "pexpect                   4.8.0              pyhd3eb1b0_3  \r\n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003  \r\n",
      "pillow                    8.4.0            py37h5aabda8_0  \r\n",
      "pip                       21.0.1           py37h06a4308_0  \r\n",
      "pkginfo                   1.7.1            py37h06a4308_0  \r\n",
      "preshed                   3.0.6                    pypi_0    pypi\r\n",
      "prometheus-client         0.12.0                   pypi_0    pypi\r\n",
      "promise                   2.3                      pypi_0    pypi\r\n",
      "prompt-toolkit            3.0.20             pyhd3eb1b0_0  \r\n",
      "protobuf                  3.19.1                   pypi_0    pypi\r\n",
      "psutil                    5.8.0            py37h27cfd23_1  \r\n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2  \r\n",
      "py-lief                   0.10.1           py37h403a769_0  \r\n",
      "pyarrow                   6.0.0                    pypi_0    pypi\r\n",
      "pyasn1                    0.4.8                    pypi_0    pypi\r\n",
      "pyasn1-modules            0.2.8                    pypi_0    pypi\r\n",
      "pycocotools               2.0.4                    pypi_0    pypi\r\n",
      "pycosat                   0.6.3            py37h27cfd23_0  \r\n",
      "pycparser                 2.20                       py_2  \r\n",
      "pydantic                  1.8.2                    pypi_0    pypi\r\n",
      "pydicom                   2.2.2                    pypi_0    pypi\r\n",
      "pygments                  2.10.0             pyhd3eb1b0_0  \r\n",
      "pyjwt                     2.3.0                    pypi_0    pypi\r\n",
      "pyopenssl                 20.0.1             pyhd3eb1b0_1  \r\n",
      "pyparsing                 2.4.7                    pypi_0    pypi\r\n",
      "pyrsistent                0.18.0                   pypi_0    pypi\r\n",
      "pysocks                   1.7.1                    py37_1  \r\n",
      "python                    3.7.11               h12debd9_0  \r\n",
      "python-dateutil           2.8.2                    pypi_0    pypi\r\n",
      "python-etcd               0.4.5                    pypi_0    pypi\r\n",
      "python-graphviz           0.18                     pypi_0    pypi\r\n",
      "python-libarchive-c       2.9                pyhd3eb1b0_1  \r\n",
      "pytorch                   1.10.0          py3.7_cuda11.3_cudnn8.2.0_0    pytorch\r\n",
      "pytorch-mutex             1.0                        cuda    pytorch\r\n",
      "pytz                      2021.3             pyhd3eb1b0_0  \r\n",
      "pywavelets                1.2.0                    pypi_0    pypi\r\n",
      "pyyaml                    5.4.1            py37h27cfd23_1  \r\n",
      "pyzmq                     22.3.0                   pypi_0    pypi\r\n",
      "qtconsole                 5.2.0                    pypi_0    pypi\r\n",
      "qtpy                      1.11.2                   pypi_0    pypi\r\n",
      "qudida                    0.0.4                    pypi_0    pypi\r\n",
      "readline                  8.1                  h27cfd23_0  \r\n",
      "requests                  2.25.1             pyhd3eb1b0_0  \r\n",
      "requests-oauthlib         1.3.0                    pypi_0    pypi\r\n",
      "rfc3987                   1.3.8                    pypi_0    pypi\r\n",
      "ripgrep                   12.1.1                        0  \r\n",
      "rsa                       4.7.2                    pypi_0    pypi\r\n",
      "ruamel_yaml               0.15.100         py37h27cfd23_0  \r\n",
      "s3transfer                0.5.0                    pypi_0    pypi\r\n",
      "scikit-image              0.18.3                   pypi_0    pypi\r\n",
      "scikit-learn              1.0.1                    pypi_0    pypi\r\n",
      "scipy                     1.7.2                    pypi_0    pypi\r\n",
      "send2trash                1.8.0                    pypi_0    pypi\r\n",
      "sentencepiece             0.1.86                   pypi_0    pypi\r\n",
      "sentry-sdk                1.4.3                    pypi_0    pypi\r\n",
      "setuptools                58.0.4           py37h06a4308_0  \r\n",
      "shortuuid                 1.0.8                    pypi_0    pypi\r\n",
      "simplejson                3.17.5                   pypi_0    pypi\r\n",
      "six                       1.16.0             pyhd3eb1b0_0  \r\n",
      "smart-open                5.2.1                    pypi_0    pypi\r\n",
      "smmap                     5.0.0                    pypi_0    pypi\r\n",
      "soupsieve                 2.2.1              pyhd3eb1b0_0  \r\n",
      "spacy                     3.2.0                    pypi_0    pypi\r\n",
      "spacy-legacy              3.0.8                    pypi_0    pypi\r\n",
      "spacy-loggers             1.0.1                    pypi_0    pypi\r\n",
      "sqlite                    3.36.0               hc218d9a_0  \r\n",
      "srsly                     2.4.2                    pypi_0    pypi\r\n",
      "strict-rfc3339            0.7                      pypi_0    pypi\r\n",
      "subprocess32              3.5.4                    pypi_0    pypi\r\n",
      "swagger-spec-validator    2.7.4                    pypi_0    pypi\r\n",
      "tensorboard               2.7.0                    pypi_0    pypi\r\n",
      "tensorboard-data-server   0.6.1                    pypi_0    pypi\r\n",
      "tensorboard-plugin-wit    1.8.0                    pypi_0    pypi\r\n",
      "tensorboardx              2.2                      pypi_0    pypi\r\n",
      "termcolor                 1.1.0                    pypi_0    pypi\r\n",
      "terminado                 0.12.1                   pypi_0    pypi\r\n",
      "testpath                  0.5.0                    pypi_0    pypi\r\n",
      "thinc                     8.0.13                   pypi_0    pypi\r\n",
      "threadpoolctl             3.0.0                    pypi_0    pypi\r\n",
      "tifffile                  2021.11.2                pypi_0    pypi\r\n",
      "tk                        8.6.10               hbc83047_0  \r\n",
      "torchelastic              0.2.0                    pypi_0    pypi\r\n",
      "torchtext                 0.11.0                     py37    pytorch\r\n",
      "torchvision               0.11.0               py37_cu113    pytorch\r\n",
      "tornado                   6.1                      pypi_0    pypi\r\n",
      "tqdm                      4.61.2             pyhd3eb1b0_1  \r\n",
      "traitlets                 5.1.0              pyhd3eb1b0_0  \r\n",
      "typer                     0.4.0                    pypi_0    pypi\r\n",
      "typing_extensions         3.10.0.2           pyh06a4308_0  \r\n",
      "tzdata                    2021a                h52ac0ba_0  \r\n",
      "urllib3                   1.26.6             pyhd3eb1b0_1  \r\n",
      "wandb                     0.12.6                   pypi_0    pypi\r\n",
      "wasabi                    0.8.2                    pypi_0    pypi\r\n",
      "wcwidth                   0.2.5              pyhd3eb1b0_0  \r\n",
      "webcolors                 1.11.1                   pypi_0    pypi\r\n",
      "webencodings              0.5.1                    pypi_0    pypi\r\n",
      "websocket-client          1.2.1                    pypi_0    pypi\r\n",
      "werkzeug                  2.0.2                    pypi_0    pypi\r\n",
      "wheel                     0.36.2             pyhd3eb1b0_0  \r\n",
      "widgetsnbextension        3.5.2                    pypi_0    pypi\r\n",
      "xz                        5.2.5                h7b6447c_0  \r\n",
      "yaml                      0.2.5                h7b6447c_0  \r\n",
      "yaspin                    2.1.0                    pypi_0    pypi\r\n",
      "zipp                      3.6.0                    pypi_0    pypi\r\n",
      "zlib                      1.2.11               h7b6447c_3  \r\n",
      "zstd                      1.4.9                haebb681_0  \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# xml library for parsing xml files\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans  \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# these are the helper libraries imported.\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/ml635e2/ml635j\r\n"
     ]
    }
   ],
   "source": [
    "# Settings:\n",
    "\n",
    "!pwd\n",
    "\n",
    "files_dir = '../ir4/trainval/'\n",
    "test_dir = '../ir4/test/'\n",
    "\n",
    "img_file_suffix = 'edit below'\n",
    "\n",
    "setwidth = 260\n",
    "#setheight = 7990\n",
    "setheight = 7500\n",
    "\n",
    "setclasses = \"edit below  self.classes = [_, 'Chip']\"\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "test_split = 0.5\n",
    "\n",
    "num_epochs = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect one image.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7990, 260, 3)\n",
      "PNG\n",
      "../ir4/trainval/inner_rim_210805T103940.png\n"
     ]
    }
   ],
   "source": [
    "imgs = [image for image in sorted(os.listdir(files_dir))\n",
    "                    if image[-4:]=='.png']\n",
    "\n",
    "img_name = imgs[1]\n",
    "image_path = os.path.join(files_dir, img_name)\n",
    "\n",
    "# reading the images and converting them to correct size and color    \n",
    "img = cv2.imread(image_path)\n",
    "imgp = Image.open(image_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "#img_res = cv2.resize(img_rgb, (width, height), cv2.INTER_AREA)\n",
    "# diving by 255\n",
    "# img_res /= 255.0\n",
    "\n",
    "print( img.shape)\n",
    "print( imgp.format)\n",
    "print( image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build the images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining the files directory and testing directory\n",
    "# files_dir = '../cast04c/train'\n",
    "# test_dir = '../cast04c/test'\n",
    "# files_dir = '../imgdata/sg/s1out/train/'\n",
    "# test_dir = '../imgdata/sg/s1out/test/'\n",
    "\n",
    "\n",
    "class AaImagesDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, files_dir, width, height, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.files_dir = files_dir\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # sorting the images for consistency\n",
    "        # To get images, the extension of the filename is checked to be jpg\n",
    "        self.imgs = [image for image in sorted(os.listdir(files_dir))\n",
    "                        if image[-4:]=='.png']\n",
    "        \n",
    "        # classes: 0 index is reserved for background\n",
    "        self.classes = [_, 'Chip']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = self.imgs[idx]\n",
    "        image_path = os.path.join(self.files_dir, img_name)\n",
    "\n",
    "        # reading the images and converting them to correct size and color    \n",
    "        img = cv2.imread(image_path)\n",
    "        imgp = Image.open(image_path)\n",
    "        #print(idx, imgp.format)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
    "        # diving by 255\n",
    "        img_res /= 255.0\n",
    "        \n",
    "        # annotation file\n",
    "        annot_filename = img_name[:-4] + '.xml'\n",
    "        annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # cv2 image gives size as height x width\n",
    "        wt = img.shape[1]\n",
    "        ht = img.shape[0]\n",
    "        \n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        for member in root.findall('object'):\n",
    "            labels.append(self.classes.index(member.find('name').text))\n",
    "            \n",
    "            # bounding box\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            \n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "            \n",
    "            xmin_corr = (xmin/wt)*self.width\n",
    "            xmax_corr = (xmax/wt)*self.width\n",
    "            ymin_corr = (ymin/ht)*self.height\n",
    "            ymax_corr = (ymax/ht)*self.height\n",
    "            \n",
    "            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
    "        \n",
    "        # convert boxes into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # getting the areas of the boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        # image_id\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = img_res,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            img_res = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "                       \n",
    "        return img_res, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset =  5 \n",
      "\n",
      "(7600, 260, 3) \n",
      " {'boxes': tensor([[1.0000e+00, 6.4338e+03, 1.7300e+02, 6.6250e+03]]), 'labels': tensor([1]), 'area': tensor([32884.5000]), 'iscrowd': tensor([0]), 'image_id': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check dataset\n",
    "dataset = AaImagesDataset(files_dir, setwidth, setheight)\n",
    "print('length of dataset = ', len(dataset), '\\n')\n",
    "\n",
    "# getting the image and target for a test index.  Feel free to change the index.\n",
    "img, target = dataset[0]\n",
    "print(img.shape, '\\n',target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to be noted -\n",
    "1. The dataset returns a tuple. The first element is the image shape and the second element is a dictionary.\n",
    "2. The image is of the size, we provided while defining the dataset and the color mode is RGB.\n",
    "3. There are four bounding boxes in the image which is evident from four lists in boxes and length of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its done! \n",
    "\n",
    "Dataset building is one of the hardest things in the notebook. If you got till here while understand all of the above, you are doing pretty good!\n",
    "\n",
    "Let's now see, what our data looks like. The function is inspired from [here](https://www.kaggle.com/kiwifairy/visualize-x-ray-image-with-bounding-boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 1.2832e+03, 1.9700e+02, 1.6617e+03])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img: [[[0.15334365 0.15334365 0.15334365]\n",
      "  [0.17184469 0.17184469 0.17184469]\n",
      "  [0.17164344 0.17164344 0.17164344]\n",
      "  ...\n",
      "  [0.20361972 0.20361972 0.20361972]\n",
      "  [0.20361972 0.20361972 0.20361972]\n",
      "  [0.19225748 0.19225748 0.19225748]]\n",
      "\n",
      " [[0.16742001 0.16742001 0.16742001]\n",
      "  [0.14509805 0.14509805 0.14509805]\n",
      "  [0.14027348 0.14027348 0.14027348]\n",
      "  ...\n",
      "  [0.19276057 0.19276057 0.19276057]\n",
      "  [0.19276057 0.19276057 0.19276057]\n",
      "  [0.19577658 0.19577658 0.19577658]]\n",
      "\n",
      " [[0.1509288  0.1509288  0.1509288 ]\n",
      "  [0.14711042 0.14711042 0.14711042]\n",
      "  [0.17848296 0.17848296 0.17848296]\n",
      "  ...\n",
      "  [0.19949691 0.19949691 0.19949691]\n",
      "  [0.2005031  0.2005031  0.2005031 ]\n",
      "  [0.19215687 0.19215687 0.19215687]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.16088751 0.16088751 0.16088751]\n",
      "  [0.16862746 0.16862746 0.16862746]\n",
      "  [0.18521671 0.18521671 0.18521671]\n",
      "  ...\n",
      "  [0.19949691 0.19949691 0.19949691]\n",
      "  [0.19607843 0.19607843 0.19607843]\n",
      "  [0.19366615 0.19366615 0.19366615]]\n",
      "\n",
      " [[0.13876419 0.13876419 0.13876419]\n",
      "  [0.14328946 0.14328946 0.14328946]\n",
      "  [0.17375645 0.17375645 0.17375645]\n",
      "  ...\n",
      "  [0.19276057 0.19276057 0.19276057]\n",
      "  [0.19969815 0.19969815 0.19969815]\n",
      "  [0.19215687 0.19215687 0.19215687]]\n",
      "\n",
      " [[0.12579206 0.12579206 0.12579206]\n",
      "  [0.12971362 0.12971362 0.12971362]\n",
      "  [0.15344426 0.15344426 0.15344426]\n",
      "  ...\n",
      "  [0.1959778  0.1959778  0.1959778 ]\n",
      "  [0.1847162  0.1847162  0.1847162 ]\n",
      "  [0.19215687 0.19215687 0.19215687]]] targ: {'boxes': tensor([[1.0000e+00, 1.2832e+03, 1.9700e+02, 1.6617e+03]]), 'labels': tensor([1]), 'area': tensor([74200.3594]), 'iscrowd': tensor([0]), 'image_id': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "# Function to visualize bounding boxes in the image\n",
    "\n",
    "def plot_img_bbox(img, target):\n",
    "    # plot the image and bboxes\n",
    "    # Bounding boxes are defined as follows: x-min y-min width height\n",
    "    fig, a = plt.subplots(1,1)\n",
    "    fig.set_size_inches(5,5)\n",
    "    a.imshow(img)\n",
    "    for box in ((target['boxes']).cpu()):\n",
    "        print(box)\n",
    "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "        rect = patches.Rectangle((x, y),\n",
    "                                 width, height,\n",
    "                                 linewidth = 2,\n",
    "                                 edgecolor = 'r',\n",
    "                                 facecolor = 'none')\n",
    "\n",
    "        # Draw the bounding box on top of the image\n",
    "        a.add_patch(rect)\n",
    "    plt.show()\n",
    "    \n",
    "# plotting the image with bboxes. Feel free to change the index\n",
    "img, target = dataset[1]\n",
    "plot_img_bbox(img, target)\n",
    "print(\"img:\",img,\"targ:\",target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we are doing great till now, as the bbox is correctly placed. \n",
    "\n",
    "Lets build the model then!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a function for loading the model. We will call it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_object_detection_model(num_classes):\n",
    "\n",
    "    # load a model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see, how easy it is to load and prepare the model using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we can apply augmentations to the image. \n",
    "\n",
    "The augmentations to object detection vary from normal augmentations becuase here we need to ensure that, bbox still aligns with the object correctly after transforming.\n",
    "\n",
    "Here I have added random flip transform, feel free to customize it as you feel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send train=True fro training transforms and False for val/test transforms\n",
    "def get_transform(train):\n",
    "    \n",
    "    if train:\n",
    "        return A.Compose([\n",
    "                            A.HorizontalFlip(0.5),\n",
    "                     # ToTensorV2 converts image to pytorch tensor without div by 255\n",
    "                            ToTensorV2(p=1.0) \n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                            ToTensorV2(p=1.0)\n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets prepare datasets and dataloaders for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = AaImagesDataset(files_dir,  setwidth, setheight, transforms= get_transform(train=True))\n",
    "dataset_test = AaImagesDataset(files_dir, setwidth, setheight, transforms= get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "# train test split\n",
    "# test_split = 0.5\n",
    "tsize = int(len(dataset)*test_split)\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=10, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train on gpu if selected.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_object_detection_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the training begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10900/1535917619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# training for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/ml635e2/ml635j/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_roi_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mrois\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_roi_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscales\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_scales\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mscales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36msetup_scales\u001b[0;34m(self, features, image_shapes)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0moriginal_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mscales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_input_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;31m# get the levels in the feature map by leveraging the fact that the network always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# downsamples by a factor of 2 at each level.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0moriginal_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mscales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_input_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;31m# get the levels in the feature map by leveraging the fact that the network always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# downsamples by a factor of 2 at each level.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36minfer_scale\u001b[0;34m(self, feature, original_size)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mpossible_scales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mpossible_scales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpossible_scales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpossible_scales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training for 10 epochs\n",
    "\n",
    "# edit me..\n",
    "#num_epochs = 3\n",
    "\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    # training for one epoch\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "    \n",
    "print('reached.end.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Comments\n",
    "\n",
    "An AP of 0.78-0.80 is not bad but perhaps we can make it even better with more augmentations, I will leave that to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicts a lot of bounding boxes per image, to take out the overlapping ones, We will use **Non Max Suppression** if you want to brush up on that, check [this](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c) out.\n",
    "\n",
    "Torchvision provides us a utility to apply nms to our predictions, lets build a function `apply_nms` using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes the original prediction and the iou threshold.\n",
    "\n",
    "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
    "    \n",
    "    # torchvision returns the indices of the bboxes to keep\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "    \n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "# function to convert a torchtensor back to PIL image\n",
    "def torch_to_pil(img):\n",
    "    return torchtrans.ToPILImage()(img).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our Generated/trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take an image from our test dataset and see, how our model does.\n",
    "\n",
    "We will first see, how many bounding boxes does our model predict compared to actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, target = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])[0]\n",
    "    \n",
    "print('predicted #boxes: ', len(prediction['labels']))\n",
    "print('real #boxes: ', len(target['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! Thats a lot of bboxes. Lets plot them and check what did it predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EXPECTED OUTPUT')\n",
    "plot_img_bbox(torch_to_pil(img), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MODEL OUTPUT')\n",
    "plot_img_bbox(torch_to_pil(img), prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model predicts a lot of bounding boxes for every apple. Lets apply nms to it and see the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_prediction = apply_nms(prediction, iou_thresh=0.8)\n",
    "print('NMS APPLIED MODEL OUTPUT')\n",
    "plot_img_bbox(torch_to_pil(img), nms_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take an image from the test set and try to predict on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pwd\n",
    "test_dataset = AaImagesDataset(test_dir,  setwidth, setheight, transforms= get_transform(train=True))\n",
    "# pick one image from the test set\n",
    "img, target = test_dataset[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])[0]\n",
    "    \n",
    "print('EXPECTED OUTPUT\\n')\n",
    "plot_img_bbox(torch_to_pil(img), target)\n",
    "print('MODEL OUTPUT\\n')\n",
    "nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
    "\n",
    "plot_img_bbox(torch_to_pil(img), nms_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model did predict a void even though it was a small amount of image data and a simple short training.\n",
    "\n",
    "But fear not, this is just a base line model here are some ideas we can improve it - \n",
    "1. Use a better model. \n",
    "   We have the option of changing the backbone of our model which at present is `resnet 50` and the fine tune it.\n",
    "2. We can change the training configurations like size of the images, optimizers and learning rate schedule.\n",
    "3. We can add more augmentations.\n",
    "   We have used the Albumentations library which has an extensive library of data augmentation functions. Feel free to explore and try them out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin.\n",
    "\n",
    "That's it for the notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
