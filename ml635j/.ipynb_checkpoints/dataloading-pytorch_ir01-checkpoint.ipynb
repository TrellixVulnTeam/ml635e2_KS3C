{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref.\n",
    "\n",
    "SHUBHANKARMOHAN · 3Y AGO · 4,758 VIEWS\n",
    "https://www.kaggle.com/code/bitthal/understanding-input-data-and-loading-with-pytorch/notebook\n",
    "https://www.kaggle.com/code/davidg1215/understanding-input-data-and-loading-with-pytorch/edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inner_rim_210805T103931.xml', 'inner_rim_210805T104018.xml', 'inner_rim_210805T104053.png', 'inner_rim_210805T104053.xml', 'inner_rim_210805T103931.png', 'inner_rim_210805T104036.png', 'inner_rim_210805T104018.png', 'inner_rim_210805T103940.xml', 'inner_rim_210805T104036.xml', 'inner_rim_210805T103940.png'] \n",
      " ['inner_rim_210805T103931.png', 'inner_rim_210805T103940.png', 'inner_rim_210805T104018.png', 'inner_rim_210805T104036.png', 'inner_rim_210805T104053.png']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "trainval = \"../ir4/trainval\"\n",
    "#print(os.listdir(trainval))\n",
    "mf = (os.listdir(trainval))\n",
    "\n",
    "imgs = [image for image in sorted(os.listdir(trainval))\n",
    "        if image[-4:]=='.png']\n",
    "print(mf, '\\n', imgs)\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files:  10\n"
     ]
    }
   ],
   "source": [
    "img_base_path = \"../ir4/trainval\"\n",
    "# Files\n",
    "images = os.listdir(img_base_path)\n",
    "print(\"Number of files: \", len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Samples from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inner_rim_210805T104053.png', 'inner_rim_210805T103940.png']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'annot_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4009/973556745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mannot_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.xml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#annot_file_path = os.path.join(trainval, annot_filename)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannot_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m### Getting Bounding Box of dog (ROI) in Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'annot_file_path' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1152 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# choose  random images to display\n",
    "images_to_display = random.choices(imgs, k=2)\n",
    "print(images_to_display)\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, img in enumerate(images_to_display):\n",
    "    ax = fig.add_subplot(8, 8, ii + 1, xticks=[], yticks=[])\n",
    "    imgi = Image.open(os.path.join(img_base_path, img))\n",
    "    plt.imshow(imgi)\n",
    "    \n",
    "annot_filename = img[:-4] + '.xml'\n",
    "#annot_file_path = os.path.join(trainval, annot_filename)\n",
    "print(annot_filename, annot_file_path)\n",
    "        \n",
    "### Getting Bounding Box of dog (ROI) in Image\n",
    "tree = ET.parse(annotation_path)\n",
    "root = tree.getroot()\n",
    "objects = root.findall('object')\n",
    "for obj in objects:\n",
    "    bndbox = obj.find('bndbox')\n",
    "    xmin = int(bndbox.find('xmin').text)\n",
    "    ymin = int(bndbox.find('ymin').text)\n",
    "    xmax = int(bndbox.find('xmax').text)\n",
    "    ymax = int(bndbox.find('ymax').text)\n",
    "bbox = (xmin, ymin, xmax, ymax)\n",
    "print(\"Bounding Box: \", bbox)\n",
    "\n",
    "# crop image\n",
    "img = img.crop(bbox)\n",
    "\n",
    "# display crop image\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "plt.imshow(img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inner_rim_210805T104018.png'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_to_display[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations\n",
    "#### The structure of the annotations are classic XML with the bbox at \"annotation/object/bndbox\".\n",
    "Structure of XML is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: ../input/annotation/Annotation/n02091244-Ibizan_hound/n02091244_2934: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../input/annotation/Annotation/n02091244-Ibizan_hound/n02091244_2934"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using annotations to crop ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose image\n",
    "image_name = 'n02105641_169.jpg'\n",
    "\n",
    "# read image\n",
    "img = Image.open(os.path.join(img_base_path, image_name))\n",
    "\n",
    "# display image\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise(extra features) can be observed in image. These will lead to poor generation as model will get confused what to produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding annotation of this particular image\n",
    "# image_name == dogBreed_number\n",
    "\n",
    "annotation_folders = os.listdir('../input/annotation/Annotation')\n",
    "breed_folder = [x for x in annotation_folders if image_name.split('_')[0] in x]\n",
    "assert len(breed_folder) == 1, \"Multiple Folders Found\"\n",
    "\n",
    "breed_folder = breed_folder[0]\n",
    "print(\"Image Folder: \", breed_folder)\n",
    "annotation_path = os.path.join('../input/annotation/Annotation', breed_folder, image_name[:-4])\n",
    "print(\"Annotation Path: \", annotation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View annotations\n",
    "#!cat ../input/annotation/Annotation/n02105641-Old_English_sheepdog/n02105641_169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting Bounding Box of dog (ROI) in Image\n",
    "tree = ET.parse(annotation_path)\n",
    "root = tree.getroot()\n",
    "objects = root.findall('object')\n",
    "for obj in objects:\n",
    "    bndbox = obj.find('bndbox')\n",
    "    xmin = int(bndbox.find('xmin').text)\n",
    "    ymin = int(bndbox.find('ymin').text)\n",
    "    xmax = int(bndbox.find('xmax').text)\n",
    "    ymax = int(bndbox.find('ymax').text)\n",
    "bbox = (xmin, ymin, xmax, ymax)\n",
    "print(\"Bounding Box: \", bbox)\n",
    "\n",
    "# crop image\n",
    "img = img.crop(bbox)\n",
    "\n",
    "# display crop image\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crops will give be having only dogs in them, so model will be able to learn better and easier.\n",
    "### A point to remember is that images may contain more than one dogs in which case annotations will have more than one bounding boxes in them (image: 'n02088364_3752.jpg'). \n",
    "Try \"!cat ../input/annotation/Annotation/n02088364-beagle/n02088364_3752\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "### Creating PyTorch DataLoader with Data Augmentation\n",
    "#### Data Augmentation help in training model better. Its like increasing size of your Dataset and prevents overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loader will use the underlying loader plus crop the image based on the annotation\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "annotation_folders = os.listdir('../input/annotation/Annotation')\n",
    "def ImageLoader(path):\n",
    "    img = datasets.folder.default_loader(path) # default loader\n",
    "    # Get bounding box\n",
    "    breed_folder = [x for x in annotation_folders if path.split('/')[-1].split('_')[0] in x][0]\n",
    "    annotation_path = os.path.join('../input/annotation/Annotation', breed_folder, path.split('/')[-1][:-4])\n",
    "\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall('object')\n",
    "    for obj in objects:\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "    bbox = (xmin, ymin, xmax, ymax)\n",
    "    \n",
    "    # return cropped image\n",
    "    img = img.crop(bbox)\n",
    "    img = img.resize((64, 64), Image.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# Data Pre-procesing and Augmentation (Experiment on your own)\n",
    "random_transforms = [transforms.ColorJitter(), transforms.RandomRotation(degrees=20)]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.CenterCrop(64),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomApply(random_transforms, p=0.3),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# The dataset (example)\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    '../input/all-dogs/',\n",
    "    loader=ImageLoader, # THE CUSTOM LOADER\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(figsize=(32, 32), ncols=8, nrows=8)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(dataset[i][0].permute(1, 2, 0).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try commenting crop line from above code to see the difference.\n",
    "#### Also code is considering only one dog per image.\n",
    "#### Like if it helps you :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
